{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datamodel_code_generator\n",
        "!pip install httpcore\n",
        "!pip install httpcore==0.15.0 httpx pymongo googletrans\n",
        "!pip install openai\n",
        "!pip install func_timeout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pDf65FDGn2q",
        "outputId": "4100358d-8ac6-4890-e6e3-4c780d034510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datamodel_code_generator\n",
            "  Downloading datamodel_code_generator-0.26.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting argcomplete<4.0,>=1.10 (from datamodel_code_generator)\n",
            "  Downloading argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting black>=19.10b0 (from datamodel_code_generator)\n",
            "  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting genson<2.0,>=1.2.1 (from datamodel_code_generator)\n",
            "  Downloading genson-1.3.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting inflect<6.0,>=4.1.0 (from datamodel_code_generator)\n",
            "  Downloading inflect-5.6.2-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting isort<6.0,>=4.3.21 (from datamodel_code_generator)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: jinja2<4.0,>=2.10.1 in /usr/local/lib/python3.10/dist-packages (from datamodel_code_generator) (3.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datamodel_code_generator) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.4.0,<3.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator) (2.9.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from datamodel_code_generator) (6.0.2)\n",
            "Requirement already satisfied: toml<1.0.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from datamodel_code_generator) (0.10.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black>=19.10b0->datamodel_code_generator) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black>=19.10b0->datamodel_code_generator)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black>=19.10b0->datamodel_code_generator)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black>=19.10b0->datamodel_code_generator) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black>=19.10b0->datamodel_code_generator) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black>=19.10b0->datamodel_code_generator) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0,>=2.10.1->datamodel_code_generator) (3.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.4.0,<3.0,>=1.9.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.4.0,<3.0,>=1.9.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator) (2.23.4)\n",
            "Collecting email-validator>=2.0.0 (from pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->pydantic[email]!=2.4.0,<3.0,>=1.9.0; python_version >= \"3.10\" and python_version < \"3.11\"->datamodel_code_generator) (3.10)\n",
            "Downloading datamodel_code_generator-0.26.1-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.5.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m896.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading genson-1.3.0-py3-none-any.whl (21 kB)\n",
            "Downloading inflect-5.6.2-py3-none-any.whl (33 kB)\n",
            "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: genson, pathspec, mypy-extensions, isort, inflect, dnspython, argcomplete, email-validator, black, datamodel_code_generator\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 7.4.0\n",
            "    Uninstalling inflect-7.4.0:\n",
            "      Successfully uninstalled inflect-7.4.0\n",
            "Successfully installed argcomplete-3.5.1 black-24.10.0 datamodel_code_generator-0.26.1 dnspython-2.7.0 email-validator-2.2.0 genson-1.3.0 inflect-5.6.2 isort-5.13.2 mypy-extensions-1.0.0 pathspec-0.12.1\n",
            "Collecting httpcore\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpcore) (2024.8.30)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6\n",
            "Collecting httpcore==0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h11<0.13,>=0.11 (from httpcore==0.15.0)\n",
            "  Downloading h11-0.12.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: sniffio==1.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (1.3.1)\n",
            "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpcore==0.15.0) (2024.8.30)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio==3.*->httpcore==0.15.0) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio==3.*->httpcore==0.15.0) (1.2.2)\n",
            "INFO: pip is looking at multiple versions of httpx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.7.0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting hstspreload (from httpx)\n",
            "  Downloading hstspreload-2024.10.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting chardet==3.* (from httpx)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna>=2.8 (from anyio==3.*->httpcore==0.15.0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting googletrans\n",
            "  Downloading googletrans-2.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from googletrans) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->googletrans) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->googletrans) (2.2.3)\n",
            "Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-py3-none-any.whl size=15761 sha256=70d1e9ba20fbb5d7a6c13dd0c9e14a8af5a54fc376cd5c42ee7adea205753b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/5f/60/c4738a8b36085696062052befbbfb65fc94d2286fb17015856\n",
            "Successfully built googletrans\n",
            "Installing collected packages: pymongo, h11, httpcore, googletrans, httpx\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.6\n",
            "    Uninstalling httpcore-1.0.6:\n",
            "      Successfully uninstalled httpcore-1.0.6\n",
            "Successfully installed googletrans-2.4.0 h11-0.12.0 httpcore-0.15.0 httpx-0.25.1 pymongo-4.10.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (0.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.12.0)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "Successfully installed jiter-0.6.1 openai-1.51.2\n",
            "Collecting func_timeout\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: func_timeout\n",
            "  Building wheel for func_timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for func_timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15077 sha256=a3ee05b506aa8af9eece1ed9012516ce77cce3fd0c87d841ed4e5dd747e96d0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/83/19/b5552bb9630e353f7c5b15be44bf10900afe1abbbfcf536afd\n",
            "Successfully built func_timeout\n",
            "Installing collected packages: func_timeout\n",
            "Successfully installed func_timeout-4.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-TeHGgMGDrA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
        "\n",
        "model_outputs = model.generate(\n",
        "    model_inputs,\n",
        "    max_new_tokens=1024,\n",
        "    top_p=0.7,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "output_token_ids = [\n",
        "    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))\n",
        "]\n",
        "\n",
        "responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "print(responses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucMrLmR3GN5k",
        "outputId": "4bae46fc-e44c-48be-a22e-cb71293d6664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "思考：回复文本中没有明显的数学计算或代码，因此不需要使用calculator或code_interpreter工具。回复文本是一个完整的句子，可以直接进行核查。使用match工具来检查回复是否与历史事实相符。\n",
            "\n",
            "行为：match(sentence=\"印度尼西亚在独立之前被荷兰殖民统治了约350年。\", context=\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from tools import *\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.generation.utils import GenerationConfig\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "def init_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map='auto', trust_remote_code=True)\n",
        "    model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_path,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "# Dump the data to a JSON lines file\n",
        "def dump_jsonl(data, output_path, append=False):\n",
        "    mode = 'a+' if append else 'w'\n",
        "    with open(output_path, mode, encoding='utf-8') as f:\n",
        "        json_record = json.dumps(data, ensure_ascii=False)\n",
        "        f.write(json_record + '\\n')\n",
        "\n",
        "# Main function for generating responses\n",
        "def generate_single_question_answer(model_path, question, answer, output_file):\n",
        "    model, tokenizer = init_model(model_path)\n",
        "\n",
        "    temp = (\"你是一个通过特定的框架检测回复文本中的幻象的智能体。下面是检测框架的详细说明。\\n\"\n",
        "            \"首先，你需要判断是否要将输入中的回复文本拆分为句子列表。 你可以使用拆分句子的工具。\"\n",
        "            \"如果需要拆分，需要对每个句子逐一进行核查；否则就对整个回复文本进行核查。\"\n",
        "            \"你可以选择适当的事实核查工具来获取用于核查的相关信息和知识然后使用匹配工具输出判断结果或者直接输出判断结果。\"\n",
        "            \"如果不使用match工具而直接输出判断结果，则需要在思考中输出label。\"\n",
        "            \"存在错误输出\\\"label = 1\\\"；不存在错误输出\\\"label = 0\\\"。\"\n",
        "            \"核查完毕后，你需要在思考中反思所有检测结果并输出label，在行为中调用get_answer()输出最终的检测结果，如果存在幻象一并输出幻象内容和证据。\\n\\n\"\n",
        "            \"分句工具：\\nsplit_text(text: str) → sentence_list\\n\"\n",
        "            \"输入是文本，该函数将文本分割成句子列表。\\n\\n\"\n",
        "            \"事实核查工具：\\nweb_search(sentence: str) → fact\\n\"\n",
        "            \"输入是一个句子，该函数使用搜索引擎来搜索相关信息。调用web_search后必须接着调用match工具来判断言判断的回复与检索到的信息是否匹配。\\n\\n\"\n",
        "            \"calculator(sentence: str, formula: str) → result, label\\n\"\n",
        "            \"输入是需要检查的公式，此函数使用计算器来获取计算结果并判断得到的结果是否与句子匹配。\"\n",
        "            \"如果匹配label为0，否则为1。有效的运算符有 +、-、*、/ 和 (, )。\"\n",
        "            \"例如，合法的输入可以是“(1 + 2) * 3”。如果输入为方程，需要将其转换为不含未知数的算式。\\n\\n\"\n",
        "            \"word_count(length: int, text: str) → count, label\\n\"\n",
        "            \"输入文本的指定字数和一段文本。该函数计算这段文本的字数并输出为count。如果字数不符合要求，输出label为1，否则为0。\\n\\n\"\n",
        "            \"code_interpreter() → label\\n\"\n",
        "            \"该函数检查代码中是否能够正确执行。如果能正确执行，输出标签为0，否则为1。\\n\\n\"\n",
        "            \"匹配工具：\\nmatch(sentence: str, context:str) → label\\n\"\n",
        "            \"输入是一个句子以及相应的上下文。上下文可以是问题和回复中的检测句子之前的内容。\"\n",
        "            \"该函数检查句子中是否存在答非所问或自相矛盾的情况。如果有，则输出标签为1，否则为0。\"\n",
        "            \"如果你认为match的输出是错误的，可以在思考中修正label，例如如果你认为match输出的\\\"label = 0\\\"是错误的，可以在思考中输出\\\"label = 1\\\"。\\n\\n\"\n",
        "            \"每次轮到你回复时，你必须严格遵循以下格式给出你的思考和行为：\"\n",
        "            \"\\\"思考：你的思考过程。\\n行为：工具调用。如match(sentence=\\\"...\\\", context=\\\"...\\\")\\\"，\"\n",
        "            \"其中思考部分是你的规划内容，行为部分必须为一个工具调用指令。\"\n",
        "            \"每次你调用工具后，我会以这种格式为你提供结果：“观察：工具的输出结果”。\")\n",
        "\n",
        "    messages = []\n",
        "    query = {\"role\": \"user\", \"content\": temp + \"问题：\" + question + \" 回复：\" + answer}\n",
        "    messages.append(query)\n",
        "\n",
        "    # Prepare the input for the model\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
        "\n",
        "    # Generate model outputs\n",
        "    model_outputs = model.generate(\n",
        "        model_inputs['input_ids'],\n",
        "        max_new_tokens=1024,\n",
        "        top_p=0.7,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Process outputs\n",
        "    output_token_ids = model_outputs[:, len(model_inputs['input_ids'][0]):]\n",
        "    responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "    print(responses)\n",
        "\n",
        "    # Rest of the logic based on responses\n",
        "    if \"行为：\" in responses:\n",
        "        action = responses.split(\"行为：\")[-1].strip(\".\").strip()\n",
        "        if \"\\n\" in action:\n",
        "            action = action.split(\"\\n\")[0]\n",
        "        responses = responses.split(\"行为：\")[0] + \"行为：\" + action\n",
        "    messages.append({\"role\": \"assistant\", \"content\": responses})\n",
        "\n",
        "    label_list = []\n",
        "    fact_list = []\n",
        "    sen_list = []\n",
        "    web_fact = \"\"\n",
        "\n",
        "    try:\n",
        "        fnum = 0\n",
        "        while True:\n",
        "            response = \"\"\n",
        "            response0 = \"观察：\"\n",
        "            action = responses.split(\"行为：\")[-1].strip(\".\").strip(\":\").strip()\n",
        "            print(action)\n",
        "            if \"split\" in action:\n",
        "                sentences = eval(action)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + str(sentences)}\n",
        "            elif \"calculator\" in action:\n",
        "                result, label = eval(action)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + result + \", label = \" + str(label)}\n",
        "                sentence = action.split(\"sentence=\")[-1].split(\"\\\"\")[1]\n",
        "                sen_list.append(sentence)\n",
        "                fact_list.append(result)\n",
        "                label_list.append(label)\n",
        "            elif \"web_search\" in action:\n",
        "                fact = eval(action)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + fact}\n",
        "                web_fact = fact\n",
        "            elif \"match\" in action:\n",
        "                label = eval(action)\n",
        "                text = \"label = \" + str(label)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + text}\n",
        "                sentence = action.split(\"sentence=\")[-1].split(\"\\\"\")[1]\n",
        "                sen_list.append(sentence)\n",
        "                fact = action.split(\"context=\")[-1].strip(\"\\\")\")\n",
        "                fact_list.append(fact)\n",
        "                label_list.append(label)\n",
        "            elif \"code_interpreter\" in action:\n",
        "                label, report = eval(action)\n",
        "                text = \"label = \" + str(label)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + text}\n",
        "                sen_list.append(answer)\n",
        "                fact = report\n",
        "                fact_list.append(fact)\n",
        "                label_list.append(label)\n",
        "            elif \"word_count\" in action:\n",
        "                text = answer\n",
        "                count, label = eval(action)\n",
        "                response = {\"role\": \"user\", \"content\": response0 + \"这段文本的字数是\" + str(count) + \"，label=\" + str(label)}\n",
        "                sen_list.append(answer)\n",
        "                label_list.append(label)\n",
        "                fact_list.append(\"这段文本的字数是\" + str(count))\n",
        "            elif \"get_answer\" in action:\n",
        "                fnum += 1\n",
        "                if fnum > 5:\n",
        "                    break\n",
        "                if not sen_list:\n",
        "                    sen_list.append(answer)\n",
        "                    if web_fact:\n",
        "                        fact_list.append(web_fact)\n",
        "                if not label_list:\n",
        "                    response = {\"role\": \"user\", \"content\": \"观察：未检测到label，请输出label并重新调用get_answer()。\"}\n",
        "                    print(response)\n",
        "                    messages.append(response)\n",
        "                    # Prepare the input for the model again\n",
        "\n",
        "\n",
        "                    # Generate model outputs again\n",
        "                    model_outputs = model.generate(\n",
        "                        messages,\n",
        "                        max_new_tokens=1024,\n",
        "                        top_p=0.7,\n",
        "                        temperature=0.7\n",
        "                    )\n",
        "\n",
        "                    # Process outputs again\n",
        "                    output_token_ids = model_outputs[:, len(model_inputs['input_ids'][0]):]\n",
        "                    responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "                    if \"行为：\" in responses:\n",
        "                        action = responses.split(\"行为：\")[-1].strip(\".\").strip()\n",
        "                        thought = responses.split(\"行为：\")[0].strip(\".\").strip()\n",
        "                        if \"label=1\" in thought:\n",
        "                            if label_list:\n",
        "                                label_list[-1] = 1\n",
        "                            else:\n",
        "                                label_list.append(1)\n",
        "                        elif \"label=0\" in thought:\n",
        "                            if label_list:\n",
        "                                label_list[-1] = 0\n",
        "                            else:\n",
        "                                label_list.append(0)\n",
        "                        if \"\\n\" in action:\n",
        "                            action = action.split(\"\\n\")[0]\n",
        "                        responses = responses.split(\"行为：\")[0] + \"行为：\" + action\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": responses})\n",
        "                    continue\n",
        "\n",
        "                final_answer = get_answer(sen_list, label_list, fact_list)\n",
        "                messages.append({\"role\": \"user\", \"content\": \"观察：\" + final_answer})\n",
        "                break\n",
        "            else:\n",
        "                print(\"No such tool, failed!\")\n",
        "                break\n",
        "\n",
        "            messages.append(response)\n",
        "            # Prepare the input for the model again\n",
        "\n",
        "\n",
        "            # Generate model outputs again\n",
        "            model_outputs = model.generate(\n",
        "                messages\n",
        "            )\n",
        "\n",
        "            # Process outputs again\n",
        "            output_token_ids = model_outputs[:, len(model_inputs['input_ids'][0]):]\n",
        "            responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "            if \"行为：\" in responses:\n",
        "                action = responses.split(\"行为：\")[-1].strip(\".\").strip()\n",
        "                thought = responses.split(\"行为：\")[0].strip(\".\").strip()\n",
        "                if \"label=1\" in thought:\n",
        "                    label_list[-1] = 1\n",
        "                elif \"label=0\" in thought:\n",
        "                    label_list[-1] = 0\n",
        "                if \"\\n\" in action:\n",
        "                    action = action.split(\"\\n\")[0]\n",
        "                responses = responses.split(\"行为：\")[0] + \"行为：\" + action\n",
        "            messages.append({\"role\": \"assistant\", \"content\": responses})\n",
        "            print(responses)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        d = {\"final_answer\": \"Error\", \"trajectory\": messages}\n",
        "        dump_jsonl(d, f\"{output_file}l\", append=True)\n",
        "        return\n",
        "\n",
        "    final_answer = final_answer.split(\"观察：\")[-1].strip()\n",
        "    d = {\"final_answer\": final_answer, \"trajectory\": messages}\n",
        "    dump_jsonl(d, output_file, append=True)\n",
        "\n",
        "# In Jupyter, you can run the following cell to pass arguments and run the script\n",
        "# Set your model path and output file path\n",
        "model_path = \"openbmb/MiniCPM3-4B\"\n",
        "output_file = \"output.json\"\n"
      ],
      "metadata": {
        "id": "QyyHDuphKGcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question-answer pair\n",
        "question = \"Is 7 a prime number?\"\n",
        "answer = \"The number 7 is not a prime number. This is because it is divisible by 7 and 7/7 = 1.\"\n",
        "\n",
        "# Run the generation process for a single question-answer pair\n",
        "generate_single_question_answer(model_path, question, answer, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMeocl-qY8IK",
        "outputId": "9bb27ca6-2784-45c3-91ee-e27b2f2b0103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 79692384 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|"
      ],
      "metadata": {
        "id": "xkYoJH2QbvQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}